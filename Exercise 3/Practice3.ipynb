{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNwlw_9ObJxP"
   },
   "source": [
    "## Practice 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZI90WX2bJxc"
   },
   "source": [
    "---\n",
    "#### Overview\n",
    "For this practice, you will be using Python and Spark to analyze the [pointwise mutual information (PMI)](http://en.wikipedia.org/wiki/Pointwise_mutual_information) of tokens in the text of Shakespeare's plays.\n",
    "\n",
    "You need to have Apache Spark installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEtWJXrebsdE"
   },
   "source": [
    "To use Spark from within a Python program, it is first necessary to tell the Python interpreter where the Spark installation is located. The code in the following cell tells Python how to find this Spark installation. This code creates SparkContext (sc) for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xnxe-BhPmbBW"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf()\\\n",
    "  .setAppName(\"YourTest\")\\\n",
    "  .setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk4QOfCtbJxe"
   },
   "source": [
    "Once Python knows where Spark is located, you can create a `SparkContext`.   All Spark commands must run within an active `SparkContext`.   The code below will create a `SparkContext`, and store a reference to the context in the variable `sc`. \n",
    "The `appName` parameter assigns a name of your choosing to the Spark jobs that are created in this context - this is useful mostly for debugging.   The `master` parameter indicates that Spark jobs will run in local mode, using two threads.   This means that your Spark jobs are not really running on a cluster, and are instead running in a single process on the local machine.   You program Spark jobs the same way whether they run in local mode or on a cluster - the main difference between local and cluster modes is, of course, performance.\n",
    "\n",
    "Next, let's test that your `SparkContext` has been set up properly by running some simple test code.   This code uses a single Spark job to estimate the value of Euler's number $e$. One way to calculate $e$ is to use the following serries by Jacob Bernoulli:\n",
    "\n",
    "$p_n = 1 - \\frac{1}{1!} + \\frac{1}{2!} - \\frac{1}{3!} + \\cdots + \\frac{(-1)^n}{n!} = \\sum_{k = 0}^n \\frac{(-1)^k}{k!}$\n",
    "\n",
    "As n tends to infinity, $p_n$ approaches $1/e$.\n",
    "\n",
    "In the following code,  `parallelize()` and `map()` are Spark *transformations*, and `reduce()` is a Spark *action*.   Study the code in the cell below, then go ahead and run it.   It should take several seconds, since a Spark job is being created and executed, and should print an estimate of $e$ when it finishes.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "octW5eVWbJxg",
    "outputId": "4a90f7dc-e7d9-49f8-c2b5-40a12b18ca8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e =  2.718281828459044\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n= 10000\n",
    "inverse_e = sc.parallelize(range(0, n)).map(lambda x: ((-1)**x) * (1 / math.factorial(x))).reduce(lambda x,y:x+y)\n",
    "e = 1 / inverse_e\n",
    "print(\"e = \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD9CXMDmbJxg"
   },
   "source": [
    "---\n",
    "#### Question 1:\n",
    "\n",
    "In the following cell, briefly explain how the example works.   What is the Spark job doing, and how is it used to estimate the value of $e$? How accurate is our estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBiT9mjQbJxh"
   },
   "source": [
    "#### Your answer to Question 1:\n",
    "\n",
    "This code is used to approximate the value of Euler's number, e. \n",
    "\n",
    "The parallelize function distributes a local Python collection to form an RDD using the given range, [0 - 10000), and allows calculation in parallel. \n",
    "\n",
    "Then the map function applies a mathematical expression, which is used to approximate $e^{-1}$, on each element of the dataset.\n",
    "\n",
    "After that, the reduce function aggregates the results by adding all the processed elements up and get a final approximation to $e^{-1}$.\n",
    "\n",
    "Lastly inversing the result yields an approximation to Euler's number.\n",
    "\n",
    "The general expression to calculate inverse_e is by summing all the elements in the expression from n = 0 to infinity. Here we only sum up the first 10000 elements. Note the values of the expression $\\frac{1}{n!}$ becomes very small after n = 10000, which are neglectable, so in the end we still get a pretty accurate approximation to $e^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viOD-n5_bJxh"
   },
   "source": [
    "---\n",
    "### Important\n",
    "\n",
    "###### The questions that follow ask you to implement functions whose prototypes are given to you. Do **NOT** change the prototypes of the functions. Do **NOT** write code outside of the functions. All necessary code should be included in the function body (except for import statements). You may declare functions inside of the function body. When marking, we will execute your code by calling the functions from an external program, which is why your code cannot rely on statements running outside functions. Please remove any call to the functions that you may have introduced for test purposes before submitting your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you should do as much as possible using RDD transformations and actions, and little-to-nothing in the driver itself. \n",
    "\n",
    "Example:\n",
    "\n",
    "`someRDD.collect()[:10]` - this is bad! It's collecting *all* of the data onto the driver when we were only looking for 10  \n",
    "`someRDD.take(10)` - much better!\n",
    "\n",
    "Extra examples of what not to do:\n",
    "\n",
    "`sc.parallelize(myFile.readlines())` - no! Have the cluster load the file using `sc.textFile()` instead!\n",
    "\n",
    "`newRDD = sc.parallelize(<some computation>.collect())` - no!  \n",
    "`newRDD = <some computation>` - yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgkmAOV9bJxi"
   },
   "source": [
    "---\n",
    "#### Question 2:\n",
    "\n",
    "Now it is your turn to write some Spark programs. Start with the simple task of counting the number of *distinct* tokens which appear in `Shakespeare.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYH29hFxyeu6"
   },
   "source": [
    "Your code should use Spark, not the Python driver code, to read `Shakespeare.txt` and do the counting.   The idea is to use Spark to give you a data-parallel alternative to the sequential Python solution you wrote for Practice 1.\n",
    "\n",
    "Write your solution for question 2 in the code cell below, by implementing the `count_distinct_tokens` function. It should use the `SparkContext` which was created previously (referenced by the variable `sc`). The function `count_distinct_tokens` must return the number of distinct tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PuFxV3z8bJxi"
   },
   "outputs": [],
   "source": [
    "# A2Q2 BOX\n",
    "from simple_tokenize import simple_tokenize\n",
    "\n",
    "# Returns the count of distinct tokens in the `Shakespeare.txt` dataset\n",
    "def count_distinct_tokens():\n",
    "    lines = sc.textFile('Shakespeare.txt')\n",
    "    counts = lines.flatMap(lambda line: simple_tokenize(line))\\\n",
    "                  .map(lambda token: (token, 1))\\\n",
    "                  .reduceByKey(lambda x, y: x + y)\\\n",
    "                  .count()\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHPRYLElbJxj"
   },
   "source": [
    "---\n",
    "#### Question 3:\n",
    "\n",
    "Next, write a Spark program that will count the number of distinct token pairs in `Shakespeare.txt`.\n",
    "\n",
    "Write your solution for question 3 by implementing the `count_distinct_pairs` function in the code cell below.   It should use the `SparkContext` which was created previously (referenced by the variable `sc`). The function `count_distinct_pairs` must return the number of distinct token pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n43vVrAebJxk"
   },
   "outputs": [],
   "source": [
    "# A2Q3 BOX\n",
    "from simple_tokenize import simple_tokenize\n",
    "\n",
    "# Returns the count of distinct pairs in the `Shakespeare.txt` dataset\n",
    "def count_distinct_pairs():\n",
    "    def token_pair(str_lst):\n",
    "        str_set = set(str_lst)\n",
    "        result = []\n",
    "        for str1 in str_set:\n",
    "            for str2 in str_set:\n",
    "                if str1 == str2:\n",
    "                    continue\n",
    "                result.append(((str1, str2), 1))\n",
    "        return result\n",
    "    lines = sc.textFile('Shakespeare.txt')\n",
    "    counts = lines.map(lambda line: simple_tokenize(line))\\\n",
    "                  .flatMap(token_pair)\\\n",
    "                  .reduceByKey(lambda x, y: x + y)\\\n",
    "                  .count()\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUUpIg1zbJxk"
   },
   "source": [
    "---\n",
    "#### Question 4:\n",
    "\n",
    "Next, write Spark code that will calculate the probability $p(x)$ for every distinct token $x$ in `Shakespeare.txt`.   Your function should return the 50 highest-probability tokens, and their counts and probabilities.\n",
    "\n",
    "Make sure that your solution calculates probabilities and identifies the 50 highest-probability tokens in a data-parallel fashion, using Spark transformations and actions.   Only the 50 highest-probability tokens (and their counts and probabilities) should be returned by Spark to your driver code.\n",
    "\n",
    "Write your solution for question 4 by implementing the `top_50_tokens_probabilities` function in the code cell below. It should use the `SparkContext` which was created previously (referenced by the variable `sc`). The function `top_50_tokens_probabilities` must return a list of 50 (probability, count, token) tuples, ordered by probability, that is, the list returned by the function should be of the form: `[(proba1, count1, token1), (proba2, count2, token2), ..., (proba50, count50, token50)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TmKHDBW9bJxl"
   },
   "outputs": [],
   "source": [
    "# A2Q4 BOX\n",
    "from simple_tokenize import simple_tokenize\n",
    "\n",
    "# Returns a list of the top 50 (probability, count, token) tuples, ordered by probability\n",
    "def top_50_tokens_probabilities():\n",
    "    lines = sc.textFile('Shakespeare.txt')\n",
    "    line_counts = lines.count()\n",
    "    result = lines.flatMap(lambda line: set(simple_tokenize(line)))\\\n",
    "                  .map(lambda token: (token, 1))\\\n",
    "                  .reduceByKey(lambda x, y: x + y)\\\n",
    "                  .map(lambda pair: (pair[1]/line_counts, pair[1], pair[0]))\\\n",
    "                  .sortBy(lambda tuple: tuple[0], ascending = False)\\\n",
    "                  .take(50)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: \"and\" is the most frequent token, and \"the\" is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7FJUeGTbJxl"
   },
   "source": [
    "---\n",
    "#### Question 5:\n",
    "\n",
    "Next, write the code for the function `PMI` that will take a positive integer threshold $T$ as an argument, and then return all token pairs that co-occur at least $T$ times in `Shakespeare.txt`.   For each such pair $(x,y)$, the function should also return PMI$(x,y)$, the co-occurrence count of the pair, the number of times $x$ appears, and the number of times $y$ appears.\n",
    "\n",
    "Write your solution for question 5 by implementing the function `PMI` in the code cell below. It should use the `SparkContext` which was created previously (referenced by the variable `sc`). The function `PMI` should return a list of ((token1, token2), pmi, co-occurrence_count, token1_count, token2_count) tuples, that is, the list returned by the function should be of the form: `[((token1, token2), pmi, cooc_count, token1_count, token2_count), (...), ((other_token1, other_token2), other_pmi, other_cooc_count, other_token1_count, other_token2_count)]`.\n",
    "\n",
    "\n",
    "As always, calculations should be done in data-parallel fashion by using Spark.\n",
    "\n",
    "Note: You're using a \"pairs\" approach here, since the key is a pair of tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MZMY3pI9bJxm"
   },
   "outputs": [],
   "source": [
    "# A2Q5 BOX\n",
    "from simple_tokenize import simple_tokenize\n",
    "from math import log\n",
    "\n",
    "# Returns a list of tuples with the following format:\n",
    "# ((token1, token2), pmi, co-occurrence_count, token1_count, token2_count)\n",
    "def PMI(threshold):\n",
    "    def token_pair(str_set):\n",
    "        result = []\n",
    "        for str1 in str_set:\n",
    "            for str2 in str_set:\n",
    "                if str1 == str2:\n",
    "                    continue\n",
    "                result.append(((str1, str2), 1))\n",
    "        return result\n",
    "    def pmi(x, y, xy, line_count):\n",
    "        p_x = x / line_count\n",
    "        p_y = y / line_count\n",
    "        p_xy = xy / line_count\n",
    "        return log(p_xy/(p_x * p_y), 10)\n",
    "    lines = sc.textFile('Shakespeare.txt')\n",
    "    line_count = lines.count()\n",
    "    tokens = lines.flatMap(lambda line: set(simple_tokenize(line)))\\\n",
    "                  .map(lambda token: (token, 1))\\\n",
    "                  .reduceByKey(lambda x, y: x + y).collect()\n",
    "    tokens_bct = sc.broadcast(dict(tokens))\n",
    "    token_pairs = lines.map(lambda line: set(simple_tokenize(line)))\\\n",
    "                       .flatMap(token_pair)\\\n",
    "                       .reduceByKey(lambda x, y: x + y)\\\n",
    "                       .filter(lambda tuple: tuple[1] >= threshold)\n",
    "    result = token_pairs.map(lambda pair: (pair[0],\n",
    "                                           pmi(tokens_bct.value[pair[0][0]],\n",
    "                                               tokens_bct.value[pair[0][1]],\n",
    "                                               pair[1],\n",
    "                                               line_count),\n",
    "                                           pair[1], \n",
    "                                           tokens_bct.value[pair[0][0]],\n",
    "                                           tokens_bct.value[pair[0][1]]))\\\n",
    "                         .collect()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5QkpXFWbJxm"
   },
   "source": [
    "---\n",
    "#### Question 6:\n",
    "\n",
    "Finally, write Spark code for the function `PMI_one_token`, that will take a positive integer threshold $T$ and a sample size $N$ as arguments. For every token $x$ in `Shakespeare.txt`, your code should find all tokens $y$ that co-occur with $x$ at least $T$ times, as well as PMI$(x,y)$ for each such pair, the co-occurrence count of the pair, the number of times $x$ appears, and the number of times $y$ appears.\n",
    "\n",
    "For each $x$, the output of your program should be equivalent to the output that would be produced by a One-Token query on $x$, with threshold $T$. Rather than producing output for all possible tokens $x$, the function should produce output only for $N$ different $x$'s, chosen uniformly at random from among the $x$'s with a non-empty list of co-occurrences.\n",
    "\n",
    "If there are fewer than $N$ different $x$'s, then the function should output all of them.  In other words, if $T$ is very large, the function would output nothing.\n",
    "\n",
    "Write your solution for question 6 by implementing the function `PMI_one_token` in the code cell below. It should use the `SparkContext` which was created previously (referenced by the variable `sc`). The function `PMI_one_token` should return a list of $N$ tuples of the form `(token, [ list_of_cooccurring_tokens ])`, where each entry in `list_of_cooccurring_tokens` is of the form `((token1, token2), pmi, cooc_count, token1_count, token2_count)`.\n",
    "\n",
    "For instance, if $N$ = 2 and the randomly selected tokens are \"if\" and \"you\", a valid format for the value returned by `PMI_one_token` would be:\n",
    "```\n",
    "[('if', [(('if', 'you'), -0.09813466615111513, 1975, 16624, 18237), (('if', 'a'), 0.03856379243802408, 1568, 16624, 10569)]), ('you', [(('you', 'if'), -0.09813466615111513, 1975, 18237, 16624)])]\n",
    "```\n",
    "\n",
    "Hint: Sampling must be done at the very last step.\n",
    "\n",
    "Hint: there is an action that returns a sample subset from an RDD. (`takeSample`)\n",
    "\n",
    "Note: You should be taking a \"stripes\" approach here. While you *can* use a pairs approach and then convert to stripes at the end, it's not as efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eUrjYZm6bJxn"
   },
   "outputs": [],
   "source": [
    "# A2Q6 BOX\n",
    "from simple_tokenize import simple_tokenize\n",
    "from math import log\n",
    "\n",
    "# Returns a list of samp_size tuples with the following format:\n",
    "# (token, [ list_of_cooccurring_tokens ])\n",
    "# where list_of_cooccurring_tokens is of the form\n",
    "# [((token1, token2), pmi, cooc_count, token1_count, token2_count), ...]\n",
    "def PMI_one_token(threshold, samp_size):\n",
    "    def token_stripe(str_set):\n",
    "        result = []\n",
    "        for str1 in str_set:\n",
    "            dict = {}\n",
    "            for str2 in str_set:\n",
    "                if str1 != str2:\n",
    "                    dict[(str1, str2)] = 1\n",
    "            result.append((str1, dict))\n",
    "        return result\n",
    "    def mergedict(d1, d2):\n",
    "        keys = set(d1) | set(d2)\n",
    "        result = {}\n",
    "        for key in keys:\n",
    "            result[key] = d1.get(key, 0) + d2.get(key, 0)\n",
    "        return result\n",
    "    def stripefilter(stripe):\n",
    "        token = stripe[0]\n",
    "        list_pairs = []\n",
    "        for pair in stripe[1].items():\n",
    "            if pair[1] >= threshold:\n",
    "                list_pairs.append(pair)\n",
    "        return (token, list_pairs)\n",
    "    lines = sc.textFile('Shakespeare.txt')\n",
    "    line_count = lines.count()\n",
    "    tokens = lines.flatMap(lambda line: set(simple_tokenize(line)))\\\n",
    "                  .map(lambda token: (token, 1))\\\n",
    "                  .reduceByKey(lambda x, y: x + y).collect()\n",
    "    tokens_bct = sc.broadcast(dict(tokens))\n",
    "    \n",
    "    token_stripes = lines.map(lambda line: set(simple_tokenize(line)))\\\n",
    "                         .flatMap(token_stripe)\\\n",
    "                         .reduceByKey(mergedict)\n",
    "    token_stripes_threshold = token_stripes.map(stripefilter).filter(lambda pair: pair[1] != [])\n",
    "    def pmi(stripe):\n",
    "        list = stripe[1]\n",
    "        x = 0\n",
    "        y = 0\n",
    "        xy = 0\n",
    "        token1 = ''\n",
    "        token2 = ''\n",
    "        result = []\n",
    "        for pair in list:\n",
    "            token1 = pair[0][0]\n",
    "            token2 = pair[0][1]\n",
    "            xy = pair[1]\n",
    "            x = tokens_bct.value[token1]\n",
    "            y = tokens_bct.value[token2]\n",
    "            p_x = x / line_count\n",
    "            p_y = y / line_count\n",
    "            p_xy = xy / line_count\n",
    "            result.append(((token1, token2), log(p_xy/(p_x * p_y), 10), xy, x, y))\n",
    "        return result\n",
    "    result = token_stripes_threshold.map(lambda stripe: (stripe[0], pmi(stripe))).takeSample(False, samp_size)\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
